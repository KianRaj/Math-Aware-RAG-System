question,answer,contexts,ground_truth,context_precision,context_recall,answer_relevancy,faithfulness,context_entity_recall
Derive the ELBO objective function step-by-step.,"[Slide 1]
Title: Derivation Overview
Bullets:
- Purpose of derivation
- Key starting equation
Script:
Explain the motivation.

[Slide 2]
Title: Step-by-Step Derivation
Bullets:
- Step 1 — ...
- Step 2 — ...
Script:
Show equations and logic.

[Slide 3]
Title: Final Result
Bullets:
- Derived equation
- Interpretation
Script:
Summarize result and relevance.

---

### Query:
Derive the ELBO objective function step-by-step.

---

Ensure:
- All math uses LaTeX ($...$ or $$...$$).
- Use only context provided from the paper.
- Do NOT invent results not in the text.
- If uncertain, say ""Not enough information in the paper.""
- Begin your answer directly with [Slide 1].
model
## Derivation Overview

This text describes a Variational Autoencoder (VAE) and its relationship to the Evidence Lower Bound (ELBO). The goal is to derive the ELBO objective function step-by-step.

**Key Starting Equation:**

```latex
log p(x) = logZp(x, z)dz
```

**Step-by-Step Derivation:**

**Step 1:**

Introduce the following equation:

```latex
log p(x) = log p(x)Zqφ(z|x)dz
```

**Step 2:**

Expand the definition of Z:

```latex
log p(x) = logZ p(x, z)qφ(z|x)dz = Eqφ(z|x) fandomslog p(x, z)qφ(z|x)dx_z
```

**Step 3:**

Split the expectation:

```latex
log p(x) = Eqφ(z|x) [log p(x, z)qφ(z|x)] + KL Divergence(qφ(z|x) ∥p(z|x))
```

**Final Result:**

The derived form of the ELBO objective function is:

```latex
log p(x) = Eqφ(z|x) [log pθ(x|z)] + KL Divergence(qφ(z|x) ∥p(z|x))
```

**Interpretation:**

This equation consists of two terms: the reconstruction term and the prior matching term. The reconstruction term measures the likelihood of the original data sample given the first-step latent. The prior matching term ensures that the final latent distribution matches the Gaussian prior.","['= Eq(x_1:T |x_0)log p(xT )pθ(x_0|x_1)q(x_1|x_0)+ logTYt=2pθ(xt−1|xt)q(xt−1|xt,x_0)\x18\x18\x18\x18q(xt|x_0)(((((q(xt−1|x_0)(53)= Eq(x_1:T |x_0)""log p(xT )pθ(x_0|x_1)\x18\x18\x18\x18q(x_1|x_0)+ log \x18\x18\x18\x18q(x_1|x_0)q(xT |x_0) + logTYt=2pθ(xt−1|xt)q(xt−1|xt, x_0)#(54)= Eq(x_1:T |x_0)""log p(xT )pθ(x_0|x_1)q(xT |x_0)+TXt=2logpθ(xt−1|xt)q(xt−1|xt, x_0)#(55)= Eq(x_1:T |x_0) [log pθ(x_0|x_1)] + Eq(x_1:T |x_0)\x14logp(xT )q(xT |x_0)\x15+TXt=2Eq(x_1:T |x_0)\x14logpθ(xt−1|xt)q(xt−1|xt, x_0)\x15(56)= Eq(x_1|x_0) [log pθ(x_0|x_1)] + Eq(xT |x_0)\x14logp(xT )q(xT |x_0)\x15+TXt=2Eq(xt,xt−1|x_0)\x14logpθ(xt−1|xt)q(xt−1|xt, x_0)\x15(57)= Eq(x_1|x_0) [log pθ(x_0|x_1)]|{z}reconstruction term−KL Divergence(q(xT |x_0) ∥p(xT ))|{z}prior matching term−TXt=2Eq(xt|x_0) [KL Divergence(q(xt−1|xt, x_0) ∥pθ(xt−1|xt))]|{z}denoising matching term(58)We have therefore successfully derived an interpretation for the Evidence Lower Bound (ELBO) that can be estimated with lowervariance, as each term is computed as an expectation of at most one random variable at a time. Thisformulation also has an elegant interpretation, which is revealed when inspecting each individual term:1. Eq(x_1|x_0) [log pθ(x_0|x_1)] can be interpreted as a reconstruction term; like its analogue in the Evidence Lower Bound (ELBO) ofa vanilla VAE, this term can be approximated and optimized using a Monte Carlo estimate.2. KL Divergence(q(xT |x_0) ∥p(xT )) represents how close the distribution of the ﬁnal noisiﬁed input is to the stan-dard Gaussian prior. It has no trainable parameters, and is also equal to zero under our assumptions.3. Eq(xt|x_0) [KL Divergence(q(xt−1|xt, x_0) ∥pθ(xt−1|xt))] is a denoising matching term. We learn desired denoisingtransition step pθ(xt−1|xt) as an approximation to tractable, ground-truth denoising transition stepq(xt−1|xt, x_0). The q(xt−1|xt, x_0) transition step can act as a ground-truth signal, since it deﬁneshow to denoise a noisy image xt with access to what the ﬁnal, completely denoised image x_0 shouldbe. This term is therefore minimized when the two denoising steps match as closely as possible, asmeasured by their KL Divergence. As a side note, one observes that in the process of both Evidence Lower Bound (ELBO) derivations (Equation 45 and Equation 58),only the Markov assumption is used; as a result these formulae will hold true for any arbitrary MarkovianHVAE.', 'Figure 4: Under our ﬁrst derivation, a VDM can be optimized by ensuring that for every intermediate xt,the posterior from the latent above it pθ(xt|xt+1) matches the Gaussian corruption of the latent before itq(xt|xt−1). In this ﬁgure, for each intermediate xt, we minimize the diﬀerence between the distributionsrepresented by the pink and green arrows. Visually, this interpretation of the Evidence Lower Bound (ELBO) is depicted in Figure 4. The cost of optimizing a VDM is primarilydominated by the third term, since we must optimize over all timesteps t. Under this derivation, all terms of the Evidence Lower Bound (ELBO) are computed as expectations, and can therefore be approxi-mated using Monte Carlo estimates. However, actually optimizing the Evidence Lower Bound (ELBO) using the terms we just derivedmight be suboptimal; because the consistency term is computed as an expectation over two random variables{xt−1, xt+1} for every timestep, the variance of its Monte Carlo estimate could potentially be higher than aterm that is estimated using only one random variable per timestep. As it is computed by summing up T −1consistency terms, the ﬁnal estimated value of the Evidence Lower Bound (ELBO) may have high variance for large T values. Let us instead try to derive a form for our Evidence Lower Bound (ELBO) where each term is computed as an expectation over onlyone random variable at a time. The key insight is that we can rewrite encoder transitions as q(xt|xt−1) =q(xt|xt−1, x_0), where the extra conditioning term is superﬂuous due to the Markov property. Then, accordingto Bayes rule, we can rewrite each transition as:q(xt|xt−1, x_0) = q(xt−1|xt, x_0)q(xt|x_0)q(xt−1|x_0)(46)Armed with this new equation, we can retry the derivation resuming from the Evidence Lower Bound (ELBO) in Equation 37:log p(x) ≥Eq(x_1:T |x_0)\x14logp(x_0:T )q(x_1:T |x_0)\x15(47)= Eq(x_1:T |x_0)""log p(xT ) QTt=1 pθ(xt−1|xt)QTt=1 q(xt|xt−1)#(48)= Eq(x_1:T |x_0)""log p(xT )pθ(x_0|x_1) QTt=2 pθ(xt−1|xt)q(x_1|x_0) QTt=2 q(xt|xt−1)#(49)= Eq(x_1:T |x_0)""log p(xT )pθ(x_0|x_1) QTt=2 pθ(xt−1|xt)q(x_1|x_0) QTt=2 q(xt|xt−1, x_0)#(50)= Eq(x_1:T |x_0)""log pθ(xT )pθ(x_0|x_1)q(x_1|x_0)+ logTYt=2pθ(xt−1|xt)q(xt|xt−1, x_0)#(51)= Eq(x_1:T |x_0)log p(xT )pθ(x_0|x_1)q(x_1|x_0)+ logTYt=2pθ(xt−1|xt)q(xt−1|xt,x_0)q(xt|x_0)q(xt−1|x_0)(52)9', 'To make the relationship with the evidence explicit, we can mathematically write:log p(x) ≥Eqφ(z|x)\x14log p(x, z)qφ(z|x)\x15(4)Here, qφ(z|x) is a ﬂexible approximate variational distribution with parameters φ that we seek to optimize. Intuitively, it can be thought of as a parameterizable model that is learned to estimate the true distributionover latent variables for given observations x; in other words, it seeks to approximate true posterior p(z|x). As we will see when exploring the Variational Autoencoder, as we increase the lower bound by tuning theparameters φ to maximize the Evidence Lower Bound (ELBO), we gain access to components that can be used to model the truedata distribution and sample from it, thus learning a generative model. For now, let us try to dive deeperinto why the Evidence Lower Bound (ELBO) is an objective we would like to maximize. Let us begin by deriving the Evidence Lower Bound (ELBO), using Equation 1:log p(x) = logZp(x, z)dz(Apply Equation 1)(5)= logZ p(x, z)qφ(z|x)qφ(z|x)dz(Multiply by 1 = qφ(z|x)qφ(z|x))(6)= log Eqφ(z|x)\x14 p(x, z)qφ(z|x)\x15(Deﬁnition of Expectation)(7)≥Eqφ(z|x)\x14log p(x, z)qφ(z|x)\x15(Apply Jensen’s Inequality)(8)In this derivation, we directly arrive at our lower bound by applying Jensen’s Inequality. However, this doesnot supply us much useful information about what is actually going on underneath the hood; crucially, thisproof gives no intuition on exactly why the Evidence Lower Bound (ELBO) is actually a lower bound of the evidence, as Jensen’sInequality handwaves it away. Furthermore, simply knowing that the Evidence Lower Bound (ELBO) is truly a lower bound of thedata does not really tell us why we want to maximize it as an objective. To better understand the relationshipbetween the evidence and the Evidence Lower Bound (ELBO), let us perform another derivation, this time using Equation 2:log p(x) = log p(x)Zqφ(z|x)dz(Multiply by 1 =Zqφ(z|x)dz)(9)=Zqφ(z|x)(log p(x))dz(Bring evidence into integral)(10)= Eqφ(z|x) [log p(x)](Deﬁnition of Expectation)(11)= Eqφ(z|x)\x14log p(x, z)p(z|x)\x15(Apply Equation 2)(12)= Eqφ(z|x)\x14log p(x, z)qφ(z|x)p(z|x)qφ(z|x)\x15(Multiply by 1 = qφ(z|x)qφ(z|x))(13)= Eqφ(z|x)\x14log p(x, z)qφ(z|x)\x15+ Eqφ(z|x)\x14log qφ(z|x)p(z|x)\x15(Split the Expectation)(14)= Eqφ(z|x)\x14log p(x, z)qφ(z|x)\x15+ KL Divergence(qφ(z|x) ∥p(z|x))(Deﬁnition of KL Divergence)(15)≥Eqφ(z|x)\x14log p(x, z)qφ(z|x)\x15(KL Divergence always ≥0)(16)From this derivation, we clearly observe from Equation 15 that the evidence is equal to the Evidence Lower Bound (ELBO) plus theKL Divergence between the approximate posterior qφ(z|x) and the true posterior p(z|x). In fact, it was thisKL Divergence term that was magically removed by Jensen’s Inequality in Equation 8 of the ﬁrst derivation. Understanding this term is the key to understanding not only the relationship between the Evidence Lower Bound (ELBO) and theevidence, but also the reason why optimizing the Evidence Lower Bound (ELBO) is an appropriate objective at all. Firstly, we now know why the Evidence Lower Bound (ELBO) is indeed a lower bound: the diﬀerence between the evidence and theEvidence Lower Bound (ELBO) is a strictly non-negative KL term, thus the value of the Evidence Lower Bound (ELBO) can never exceed the evidence.3', 'It is called an autoencoder because it is reminiscent of a traditional au-toencoder model, where input data is trained to predict itself after undergoing an intermediate bottleneckingrepresentation step. To make this connection explicit, let us dissect the Evidence Lower Bound (ELBO) term further:Eqφ(z|x)\x14log p(x, z)qφ(z|x)\x15= Eqφ(z|x)\x14log pθ(x|z)p(z)qφ(z|x)\x15(Chain Rule of Probability)(17)= Eqφ(z|x) [log pθ(x|z)] + Eqφ(z|x)\x14logp(z)qφ(z|x)\x15(Split the Expectation)(18)= Eqφ(z|x) [log pθ(x|z)]|{z}reconstruction term−KL Divergence(qφ(z|x) ∥p(z))|{z}prior matching term(Deﬁnition of KL Divergence) (19)In this case, we learn an intermediate bottlenecking distribution qφ(z|x) that can be treated as an encoder; ittransforms inputs into a distribution over possible latents. Simultaneously, we learn a deterministic functionpθ(x|z) to convert a given latent vector z into an observation x, which can be interpreted as a decoder. The two terms in Equation 19 each have intuitive descriptions: the ﬁrst term measures the reconstructionlikelihood of the decoder from our variational distribution; this ensures that the learned distribution ismodeling eﬀective latents that the original data can be regenerated from. The second term measures howsimilar the learned variational distribution is to a prior belief held over latent variables. Minimizing thisterm encourages the encoder to actually learn a distribution rather than collapse into a Dirac delta function. Maximizing the Evidence Lower Bound (ELBO) is thus equivalent to maximizing its ﬁrst term and minimizing its second term.4', 'Note that our encoder distributions q(xt|xt−1) are no longer parameterized by φ, as they are completelymodeled as Gaussians with deﬁned mean and variance parameters at each timestep. Therefore, in a VDM, weare only interested in learning conditionals pθ(xt−1|xt), so that we can simulate new data. After optimizingthe VDM, the sampling procedure is as simple as sampling Gaussian noise from p(xT ) and iteratively runningthe denoising transitions pθ(xt−1|xt) for T steps to generate a novel x_0. Like any HVAE, the VDM can be optimized by maximizing the Evidence Lower Bound (ELBO), which can be derived as:log p(x) = logZp(x_0:T )dx_1:T(34)= logZ p(x_0:T )q(x_1:T |x_0)q(x_1:T |x_0)dx_1:T(35)= log Eq(x_1:T |x_0)\x14p(x_0:T )q(x_1:T |x_0)\x15(36)≥Eq(x_1:T |x_0)\x14logp(x_0:T )q(x_1:T |x_0)\x15(37)= Eq(x_1:T |x_0)""log p(xT ) QTt=1 pθ(xt−1|xt)QTt=1 q(xt|xt−1)#(38)= Eq(x_1:T |x_0)""log p(xT )pθ(x_0|x_1) QTt=2 pθ(xt−1|xt)q(xT |xT −1) QT −1t=1 q(xt|xt−1)#(39)= Eq(x_1:T |x_0)""log p(xT )pθ(x_0|x_1) QT −1t=1 pθ(xt|xt+1)q(xT |xT −1) QT −1t=1 q(xt|xt−1)#(40)= Eq(x_1:T |x_0)\x14log p(xT )pθ(x_0|x_1)q(xT |xT −1)\x15+ Eq(x_1:T |x_0)""logT −1Yt=1pθ(xt|xt+1)q(xt|xt−1)#(41)= Eq(x_1:T |x_0) [log pθ(x_0|x_1)] + Eq(x_1:T |x_0)\x14logp(xT )q(xT |xT −1)\x15+ Eq(x_1:T |x_0)""T −1Xt=1log pθ(xt|xt+1)q(xt|xt−1)#(42)= Eq(x_1:T |x_0) [log pθ(x_0|x_1)] + Eq(x_1:T |x_0)\x14logp(xT )q(xT |xT −1)\x15+T −1Xt=1Eq(x_1:T |x_0)\x14log pθ(xt|xt+1)q(xt|xt−1)\x15(43)= Eq(x_1|x_0) [log pθ(x_0|x_1)] + Eq(xT −1,xT |x_0)\x14logp(xT )q(xT |xT −1)\x15+T −1Xt=1Eq(xt−1,xt,xt+1|x_0)\x14log pθ(xt|xt+1)q(xt|xt−1)\x15(44)= Eq(x_1|x_0) [log pθ(x_0|x_1)]|{z}reconstruction term−Eq(xT −1|x_0) [KL Divergence(q(xT |xT −1) ∥p(xT ))]|{z}prior matching term−T −1Xt=1Eq(xt−1,xt+1|x_0) [KL Divergence(q(xt|xt−1) ∥pθ(xt|xt+1))]|{z}consistency term(45)The derived form of the Evidence Lower Bound (ELBO) can be interpreted in terms of its individual components:1. Eq(x_1|x_0) [log pθ(x_0|x_1)] can be interpreted as a reconstruction term, predicting the log probability ofthe original data sample given the ﬁrst-step latent. This term also appears in a vanilla VAE, and canbe trained similarly.2. Eq(xT −1|x_0) [KL Divergence(q(xT |xT −1) ∥p(xT ))] is a prior matching term; it is minimized when the ﬁnal latentdistribution matches the Gaussian prior. This term requires no optimization, as it has no trainableparameters; furthermore, as we have assumed a large enough T such that the ﬁnal distribution isGaussian, this term eﬀectively becomes zero.3. Eq(xt−1,xt+1|x_0) [KL Divergence(q(xt|xt−1) ∥pθ(xt|xt+1))] is a consistency term; it endeavors to make the distri-bution at xt consistent, from both forward and backward processes. That is, a denoising step from anoisier image should match the corresponding noising step from a cleaner image, for every intermediatetimestep; this is reﬂected mathematically by the KL Divergence. This term is minimized when we trainpθ(xt|xt+1) to match the Gaussian distribution q(xt|xt−1), which is deﬁned in Equation 31.8']",The ELBO is given by E_{q(z|x)}[log p(x|z)] - KL(q(z|x) || p(z)). The derivation involves applying Jensen's inequality to log p(x).,0.99999999998,1.0,0.9536622136052726,0.8,0.1999999996
Summarize the key equation for the KL Divergence.,"[Slide 1]
Title: Derivation Overview
Bullets:
- Purpose of derivation
- Key starting equation
Script:
Explain the motivation.

[Slide 2]
Title: Step-by-Step Derivation
Bullets:
- Step 1 — ...
- Step 2 — ...
Script:
Show equations and logic.

[Slide 3]
Title: Final Result
Bullets:
- Derived equation
- Interpretation
Script:
Summarize result and relevance.

---

### Query:
Summarize the key equation for the KL Divergence.

---

Ensure:
- All math uses LaTeX ($...$ or $$...$$).
- Use only context provided from the paper.
- Do NOT invent results not in the text.
- If uncertain, say ""Not enough information in the paper.""
- Begin your answer directly with [Slide 1].
model
**[Slide 1]**

**Title:** Derivation Overview

**Bullets:**

- Purpose of derivation: To derive an interpretation for the Evidence Lower Bound (ELBO) that can be estimated with lowervariance.

**Script:**

The text does not provide information about the KL Divergence equation, therefore I cannot complete this task.

**[Slide 2]**

**Title:** Not Enough Information

**Script:**

There is not enough information in the text to derive the key equation for the KL Divergence, therefore I cannot complete this task.","['Recall that the KL Divergence between two Gaussian distributions is:KL Divergence(N(x; µx, Σx) ∥N(y; µy, Σy)) = 12\x14log |Σy||Σx| −d + tr(Σ−1y Σx) + (µy −µx)T Σ−1y (µy −µx)\x15(86)In our case, where we can set the variances of the two Gaussians to match exactly, optimizing the KLDivergence term reduces to minimizing the diﬀerence between the means of the two distributions:arg minθDKL(q(xt−1|xt, x_0) ∥pθ(xt−1|xt))= arg minθDKL(N(xt−1; µq, Σq(t)) ∥N(xt−1; µθ, Σq(t)))(87)= arg minθ12\x14log |Σq(t)||Σq(t)| −d + tr(Σq(t)−1Σq(t)) + (µθ −µq)T Σq(t)−1(µθ −µq)\x15(88)= arg minθ12\x02log 1 −d + d + (µθ −µq)T Σq(t)−1(µθ −µq)\x03(89)= arg minθ12\x02(µθ −µq)T Σq(t)−1(µθ −µq)\x03(90)= arg minθ12h(µθ −µq)T ', '= Eq(x_1:T |x_0)log p(xT )pθ(x_0|x_1)q(x_1|x_0)+ logTYt=2pθ(xt−1|xt)q(xt−1|xt,x_0)\x18\x18\x18\x18q(xt|x_0)(((((q(xt−1|x_0)(53)= Eq(x_1:T |x_0)""log p(xT )pθ(x_0|x_1)\x18\x18\x18\x18q(x_1|x_0)+ log \x18\x18\x18\x18q(x_1|x_0)q(xT |x_0) + logTYt=2pθ(xt−1|xt)q(xt−1|xt, x_0)#(54)= Eq(x_1:T |x_0)""log p(xT )pθ(x_0|x_1)q(xT |x_0)+TXt=2logpθ(xt−1|xt)q(xt−1|xt, x_0)#(55)= Eq(x_1:T |x_0) [log pθ(x_0|x_1)] + Eq(x_1:T |x_0)\x14logp(xT )q(xT |x_0)\x15+TXt=2Eq(x_1:T |x_0)\x14logpθ(xt−1|xt)q(xt−1|xt, x_0)\x15(56)= Eq(x_1|x_0) [log pθ(x_0|x_1)] + Eq(xT |x_0)\x14logp(xT )q(xT |x_0)\x15+TXt=2Eq(xt,xt−1|x_0)\x14logpθ(xt−1|xt)q(xt−1|xt, x_0)\x15(57)= Eq(x_1|x_0) [log pθ(x_0|x_1)]|{z}reconstruction term−KL Divergence(q(xT |x_0) ∥p(xT ))|{z}prior matching term−TXt=2Eq(xt|x_0) [KL Divergence(q(xt−1|xt, x_0) ∥pθ(xt−1|xt))]|{z}denoising matching term(58)We have therefore successfully derived an interpretation for the Evidence Lower Bound (ELBO) that can be estimated with lowervariance, as each term is computed as an expectation of at most one random variable at a time. Thisformulation also has an elegant interpretation, which is revealed when inspecting each individual term:1. Eq(x_1|x_0) [log pθ(x_0|x_1)] can be interpreted as a reconstruction term; like its analogue in the Evidence Lower Bound (ELBO) ofa vanilla VAE, this term can be approximated and optimized using a Monte Carlo estimate.2. KL Divergence(q(xT |x_0) ∥p(xT )) represents how close the distribution of the ﬁnal noisiﬁed input is to the stan-dard Gaussian prior. It has no trainable parameters, and is also equal to zero under our assumptions.3. Eq(xt|x_0) [KL Divergence(q(xt−1|xt, x_0) ∥pθ(xt−1|xt))] is a denoising matching term. We learn desired denoisingtransition step pθ(xt−1|xt) as an approximation to tractable, ground-truth denoising transition stepq(xt−1|xt, x_0). The q(xt−1|xt, x_0) transition step can act as a ground-truth signal, since it deﬁneshow to denoise a noisy image xt with access to what the ﬁnal, completely denoised image x_0 shouldbe. This term is therefore minimized when the two denoising steps match as closely as possible, asmeasured by their KL Divergence. As a side note, one observes that in the process of both Evidence Lower Bound (ELBO) derivations (Equation 45 and Equation 58),only the Markov assumption is used; as a result these formulae will hold true for any arbitrary MarkovianHVAE.', 'Three Equivalent InterpretationsAs we previously proved, a Variational Diﬀusion Model can be trained by simply learning a neural networkto predict the original natural image x_0 from an arbitrary noised version xt and its time index t. However,x_0 has two other equivalent parameterizations, which leads to two further interpretations for a VDM. Firstly, we can utilize the reparameterization trick. In our derivation of the form of q(xt|x_0), we can rearrangeEquation 69 to show that:x_0 = xt −√1 −¯αtϵ0√¯αt(115)Plugging this into our previously derived true denoising transition mean µq(xt, x_0), we can rederive as:µq(xt, x_0) =√αt(1 −¯αt−1)xt + √¯αt−1(1 −αt)x_01 −¯αt(116)=√αt(1 −¯αt−1)xt + √¯αt−1(1 −αt) xt−√1−¯αtϵ0√¯αt_1 −¯αt(117)=√αt(1 −¯αt−1)xt + (1 −αt) xt−√1−¯αtϵ0√αt_1 −¯αt(118)=√αt(1 −¯αt−1)xt_1 −¯αt+ (1 −αt)xt(1 −¯αt)√αt−(1 −αt)√1 −¯αtϵ0(1 −¯αt)√αt(119)=\x12√αt(1 −¯αt−1)1 −¯αt+1 −αt(1 −¯αt)√αt\x13xt −(1 −αt)√1 −¯αt(1 −¯αt)√αtϵ0(120)=\x12αt(1 −¯αt−1)(1 −¯αt)√αt+1 −αt(1 −¯αt)√αt\x13xt −1 −αt√1 −¯αt√αtϵ0(121)= αt −¯αt + 1 −αt(1 −¯αt)√αtxt −1 −αt√1 −¯αt√αtϵ0(122)=1 −¯αt(1 −¯αt)√αtxt −1 −αt√1 −¯αt√αtϵ0(123)=1√αtxt −1 −αt√1 −¯αt√αtϵ0(124)Therefore, we can set our approximate denoising transition mean µθ(xt, t) as:µθ(xt, t) =1√αtxt −1 −αt√1 −¯αt√αtˆϵθ(xt, t)(125)and the corresponding optimization problem becomes:arg minθDKL(q(xt−1|xt, x_0) ∥pθ(xt−1|xt))= arg minθDKL(N (xt−1; µq, Σq (t)) ∥N (xt−1; µθ, Σq (t)))(126)= arg minθ12σ2q(t)""\r\r\r\r1√αtxt −1 −αt√1 −¯αt√αtˆϵθ(xt, t) −1√αtxt +1 −αt√1 −¯αt√αtϵ0\r\r\r\r22#(127)= arg minθ12σ2q(t)""\r\r\r\r1 −αt√1 −¯αt√αtϵ0 −1 −αt√1 −¯αt√αtˆϵθ(xt, t)\r\r\r\r22#(128)= arg minθ12σ2q(t)""\r\r\r\r1 −αt√1 −¯αt√αt(ϵ0 −ˆϵθ(xt, t))\r\r\r\r22#(129)= arg minθ12σ2q(t)(1 −αt)2(1 −¯αt)αth∥ϵ0 −ˆϵθ(xt, t)∥22i(130)15', 'It is called an autoencoder because it is reminiscent of a traditional au-toencoder model, where input data is trained to predict itself after undergoing an intermediate bottleneckingrepresentation step. To make this connection explicit, let us dissect the Evidence Lower Bound (ELBO) term further:Eqφ(z|x)\x14log p(x, z)qφ(z|x)\x15= Eqφ(z|x)\x14log pθ(x|z)p(z)qφ(z|x)\x15(Chain Rule of Probability)(17)= Eqφ(z|x) [log pθ(x|z)] + Eqφ(z|x)\x14logp(z)qφ(z|x)\x15(Split the Expectation)(18)= Eqφ(z|x) [log pθ(x|z)]|{z}reconstruction term−KL Divergence(qφ(z|x) ∥p(z))|{z}prior matching term(Deﬁnition of KL Divergence) (19)In this case, we learn an intermediate bottlenecking distribution qφ(z|x) that can be treated as an encoder; ittransforms inputs into a distribution over possible latents. Simultaneously, we learn a deterministic functionpθ(x|z) to convert a given latent vector z into an observation x, which can be interpreted as a decoder. The two terms in Equation 19 each have intuitive descriptions: the ﬁrst term measures the reconstructionlikelihood of the decoder from our variational distribution; this ensures that the learned distribution ismodeling eﬀective latents that the original data can be regenerated from. The second term measures howsimilar the learned variational distribution is to a prior belief held over latent variables. Minimizing thisterm encourages the encoder to actually learn a distribution rather than collapse into a Dirac delta function. Maximizing the Evidence Lower Bound (ELBO) is thus equivalent to maximizing its ﬁrst term and minimizing its second term.4', 'Figure 5: Depicted is an alternate, lower-variance method to optimize a VDM; we compute the form ofground-truth denoising step q(xt−1|xt, x_0) using Bayes rule, and minimize its KL Divergence with ourapproximate denoising step pθ(xt−1|xt). This is once again denoted visually by matching the distributionsrepresented by the green arrows with those of the pink arrows. Artistic liberty is at play here; in the fullpicture, each pink arrow must also stem from x_0, as it is also a conditioning term. Then, the form of q(xt|x_0) can be recursively derived through repeated applications of the reparameterizationtrick. Suppose that we have access to 2T random noise variables {ϵ∗t , ϵt}Tt=0iid∼N(ϵ; 0, I). Then, for anarbitrary sample xt ∼q(xt|x_0), we can rewrite it as:xt = √αtxt−1 +√1 −αtϵ∗t−1(61)= √αt\x10√αt−1xt−2 +p_1 −αt−1ϵ∗t−2\x11+√1 −αtϵ∗t−1(62)= √αtαt−1xt−2 +pαt −αtαt−1ϵ∗t−2 +√1 −αtϵ∗t−1(63)= √αtαt−1xt−2 +qpαt −αtαt−12 +√1 −αt_2ϵt−2(64)= √αtαt−1xt−2 +pαt −αtαt−1 + 1 −αtϵt−2(65)= √αtαt−1xt−2 +p_1 −αtαt−1ϵt−2(66)= . ..(67)=vuuttYi=1αix_0 +vuut_1 −tYi=1αiϵ0(68)= √¯αtx_0 +√1 −¯αtϵ0(69)∼N(xt; √¯αtx_0, (1 −¯αt) I)(70)where in Equation 64 we have utilized the fact that the sum of two independent Gaussian random variablesremains a Gaussian with mean being the sum of the two means, and variance being the sum of the twovariances. Interpreting √1 −αtϵ∗t−1 as a sample from Gaussian N(0, (1 −αt)I), and √αt −αtαt−1ϵ∗t−2 asa sample from Gaussian N(0, (αt −αtαt−1)I), we can then treat their sum as a random variable sampledfrom Gaussian N(0, (1 −αt + αt −αtαt−1)I) = N(0, (1 −αtαt−1)I). A sample from this distribution canthen be represented using the reparameterization trick as √1 −αtαt−1ϵt−2, as in Equation 66.11']",The KL Divergence measures the difference between q(z|x) and p(z) and is expressed as E_{q(z|x)}[log q(z|x) - log p(z)].,0.249999999975,1.0,0.0,0.125,0.249999999375
Explain how diffusion models relate to variational inference.,"[Slide 1].
model
**[Slide 1]**

**Variational Diﬀusion Models and Variational Inference**

Variational diﬀusion models (VDMs) are a type of generative model that are closely related to variational inference.

**Key connections:**

- **Score-based generative models:** VDMs are score-based generative models, which means that they learn the score function of the energy-based model as a neural network.
- **Markovian hierarchical variational autoencoders (HVAE):** VDMs are a special case of HVAE, which are models that learn a latent representation of the data by iteratively denoising it.
- **Evidence Lower Bound (ELBO):** VDMs use the ELBO technique to learn the score function.
- **Langevin dynamics sampling:** VDMs use annealed Langevin dynamics sampling to generate samples from the posterior distribution.

**Therefore, VDMs are a type of generative model that are closely related to variational inference techniques.**","['First, we derive Variational DiﬀusionModels as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptionsenable tractable computation and scalable optimization of the Evidence Lower Bound (ELBO). We then prove that optimizing a VDMboils down to learning a neural network to predict one of three potential objectives: the original source imagefrom any arbitrary noisiﬁcation of it, the original source noise from any arbitrarily noisiﬁed image, or thescore function of a noisiﬁed image at any arbitrary noise level. Then, we dive deeper into what it means tolearn the score function, and connect it explicitly with the perspective of Score-based Generative Modeling. Lastly, we cover how to learn a conditional distribution using diﬀusion models. In summary, diﬀusion models have shown incredible capabilities as generative models; indeed, they powerthe current state-of-the-art models on text-conditioned image generation such as Imagen and DALL-E 2. Furthermore, the mathematics that enable these models are exceedingly elegant. However, there still remaina few drawbacks to consider:• It is unlikely that this is how we, as humans, naturally model and generate data; we do not generatesamples as random noise that we iteratively denoise.• The VDM does not produce interpretable latents. Whereas a VAE would hopefully learn a structuredlatent space through the optimization of its encoder, in a VDM the encoder at each timestep is alreadygiven as a linear Gaussian model and cannot be optimized ﬂexibly. Therefore, the intermediate latentsare restricted as just noisy versions of the original input.• The latents are restricted to the same dimensionality as the original input, further frustrating eﬀortsto learn meaningful, compressed latent structure.• Sampling is an expensive procedure, as multiple denoising steps must be run under both formulations. Recall that one of the restrictions is that a large enough number of timesteps T is chosen to ensure theﬁnal latent is completely Gaussian noise; during sampling we must iterate over all these timesteps togenerate a sample. As a ﬁnal note, the success of diﬀusion models highlights the power of Hierarchical VAEs as a generativemodel.', 'Score-based generative models are highly related; instead of learning to model the energy function itself, theylearn the score of the energy-based model as a neural network. In this work we explore and review diﬀusionmodels, which as we will demonstrate, have both likelihood-based and score-based interpretations. Weshowcase the math behind such models in excruciating detail, with the aim that anyone can follow alongand understand what diﬀusion models are and how they work. Background: Evidence Lower Bound (ELBO), VAE, and Hierarchical VAEFor many modalities, we can think of the data we observe as represented or generated by an associatedunseen latent variable, which we can denote by random variable z. The best intuition for expressing thisidea is through Plato’s Allegory of the Cave. In the allegory, a group of people are chained inside a cave theirentire life and can only see the two-dimensional shadows projected onto a wall in front of them, which aregenerated by unseen three-dimensional objects passed before a ﬁre. To such people, everything they observeis actually determined by higher-dimensional abstract concepts that they can never behold. Analogously, the objects that we encounter in the actual world may also be generated as a function ofsome higher-level representations; for example, such representations may encapsulate abstract propertiessuch as color, size, shape, and more. Then, what we observe can be interpreted as a three-dimensionalprojection or instantiation of such abstract concepts, just as what the cave people observe is actually atwo-dimensional projection of three-dimensional objects. Whereas the cave people can never see (or evenfully comprehend) the hidden objects, they can still reason and draw inferences about them; in a similarway, we can approximate latent representations that describe the data we observe. Whereas Plato’s Allegory illustrates the idea behind latent variables as potentially unobservable representa-tions that determine observations, a caveat of this analogy is that in generative modeling, we generally seekto learn lower-dimensional latent representations rather than higher-dimensional ones.', 'Figure 2: A Markovian Hierarchical Variational Autoencoder with T hierarchical latents. The generativeprocess is modeled as a Markov chain, where each latent zt is generated only from the previous latent zt+1.decoding each latent zt only conditions on previous latent zt+1. Intuitively, and visually, this can be seenas simply stacking VAEs on top of each other, as depicted in Figure 2; another appropriate term describingthis model is a Recursive VAE. Mathematically, we represent the joint distribution and the posterior of aMarkovian HVAE as:p(x, z_1:T ) = p(zT )pθ(x|z_1)TYt=2pθ(zt−1|zt)(23)qφ(z_1:T |x) = qφ(z_1|x)TYt=2qφ(zt|zt−1)(24)Then, we can easily extend the Evidence Lower Bound (ELBO) to be:log p(x) = logZp(x, z_1:T )dz_1:T(Apply Equation 1)(25)= logZ p(x, z_1:T )qφ(z_1:T |x)qφ(z_1:T |x)dz_1:T(Multiply by 1 = qφ(z_1:T |x)qφ(z_1:T |x))(26)= log Eqφ(z_1:T |x)\x14 p(x, z_1:T )qφ(z_1:T |x)\x15(Deﬁnition of Expectation)(27)≥Eqφ(z_1:T |x)\x14log p(x, z_1:T )qφ(z_1:T |x)\x15(Apply Jensen’s Inequality)(28)We can then plug our joint distribution (Equation 23) and posterior (Equation 24) into Equation 28 toproduce an alternate form:Eqφ(z_1:T |x)\x14log p(x, z_1:T )qφ(z_1:T |x)\x15= Eqφ(z_1:T |x)""log p(zT )pθ(x|z_1) QTt=2 pθ(zt−1|zt)qφ(z_1|x) QTt=2 qφ(zt|zt−1)#(29)As we will show below, when we investigate Variational Diﬀusion Models, this objective can be furtherdecomposed into interpretable components. Variational Diﬀusion ModelsThe easiest way to think of a Variational Diﬀusion Model (VDM) [4, 5, 6] is simply as a Markovian Hierar-chical Variational Autoencoder with three key restrictions:• The latent dimension is exactly equal to the data dimension• The structure of the latent encoder at each timestep is not learned; it is pre-deﬁned as a linear Gaussianmodel. In other words, it is a Gaussian distribution centered around the output of the previous timestep• The Gaussian parameters of the latent encoders vary over time in such a way that the distribution ofthe latent at ﬁnal timestep T is a standard Gaussian_6', 'Understanding Diﬀusion Models: A Uniﬁed PerspectiveCalvin LuoGoogle Research, Brain Teamcalvinluo@google.comAugust 26, 2022ContentsIntroduction: Generative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..1Background: Evidence Lower Bound (ELBO), VAE, and Hierarchical VAE . . . . . . . . . . . . . . . . . . . . . . ..2Evidence Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..2Variational Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..4Hierarchical Variational Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . ..5Variational Diﬀusion Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..6Learning Diﬀusion Noise Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..14Three Equivalent Interpretations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..15Score-based Generative Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..17Guidance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .', 'Formally, we can choose a positive sequence of noise levels {σt}Tt=1 and deﬁne a sequence of progressivelyperturbed data distributions:pσt(xt) =Zp(x)N(xt; x, σ2t I)dx(160)Then, a neural network sθ(x, t) is learned using score matching to learn the score function for all noise levelssimultaneously:arg minθTXt=1λ(t)Epσt(xt)h∥sθ(x, t) −∇log pσt(xt)∥22i(161)where λ(t) is a positive weighting function that conditions on noise level t. Note that this objective almostexactly matches the objective derived in Equation 148 to train a Variational Diﬀusion Model. Furthermore,the authors propose annealed Langevin dynamics sampling as a generative procedure, in which samples areproduced by running Langevin dynamics for each t = T, T −1, ..., 2, 1 in sequence. The initialization is chosenfrom some ﬁxed prior (such as uniform), and each subsequent sampling step starts from the ﬁnal samplesof the previous simulation. Because the noise levels steadily decrease over timesteps t, and we reduce thestep size over time, the samples eventually converge into a true mode. This is directly analogous to thesampling procedure performed in the Markovian HVAE interpretation of a Variational Diﬀusion Model,where a randomly initialized data vector is iteratively reﬁned over decreasing noise levels. Therefore, we have established an explicit connection between Variational Diﬀusion Models and Score-basedGenerative Models, both in their training objectives and sampling procedures. One question is how to naturally generalize diﬀusion models to an inﬁnite number of timesteps. Under theMarkovian HVAE view, this can be interpreted as extending the number of hierarchies to inﬁnity T →∞. It is clearer to represent this from the equivalent score-based generative model perspective; under an inﬁnitenumber of noise scales, the perturbation of an image over continuous time can be represented as a stochasticprocess, and therefore described by a stochastic diﬀerential equation (SDE). Sampling is then performed byreversing the SDE, which naturally requires estimating the score function at each continuous-valued noiselevel [10]. Diﬀerent parameterizations of the SDE essentially describe diﬀerent perturbation schemes overtime, enabling ﬂexible modeling of the noising procedure [6]. GuidanceSo far, we have focused on modeling just the data distribution p(x).']",Diffusion models can be interpreted as hierarchical VAEs trained via a variational lower bound.,0.99999999998,1.0,0.9559966278059772,0.7777777777777778,0.0
What is the role of the reparameterization trick in this paper?,"[Slide 1].
model
**[Slide 1]**

**Reparameterization Trick in the Paper**

The reparameterization trick is used in the paper to address a problem that arises when optimizing the Evidence Lower Bound (ELBO) for a Variational Autoencoder (VAE). Specifically, the ELBO involves computing the KL divergence between the approximate posterior distribution and the prior distribution. However, the KL divergence is typically non-differentiable due to the stochastic nature of the sampling procedure used to generate samples from the approximate posterior.

The reparameterization trick overcomes this problem by rewriting the approximate posterior distribution as a deterministic function of a noise variable, which allows for the optimization of the non-stochastic terms through gradient descent. This technique is applicable to Gaussian distributions, and it is used in the paper to reparameterize the approximate posterior distribution.","['By combining ourparameterization of SNR in Equation 111 with our deﬁnition of SNR in Equation 109, we can also explicitlyderive elegant forms for the value of ¯αt as well as for the value of 1 −¯αt:¯αt_1 −¯αt= exp(−ωη(t))(112)∴¯αt = sigmoid(−ωη(t))(113)∴1 −¯αt = sigmoid(ωη(t))(114)These terms are necessary for a variety of computations; for example, during optimization, they are used tocreate arbitrarily noisy xt from input x_0 using the reparameterization trick, as derived in Equation 69.14', 'Figure 5: Depicted is an alternate, lower-variance method to optimize a VDM; we compute the form ofground-truth denoising step q(xt−1|xt, x_0) using Bayes rule, and minimize its KL Divergence with ourapproximate denoising step pθ(xt−1|xt). This is once again denoted visually by matching the distributionsrepresented by the green arrows with those of the pink arrows. Artistic liberty is at play here; in the fullpicture, each pink arrow must also stem from x_0, as it is also a conditioning term. Then, the form of q(xt|x_0) can be recursively derived through repeated applications of the reparameterizationtrick. Suppose that we have access to 2T random noise variables {ϵ∗t , ϵt}Tt=0iid∼N(ϵ; 0, I). Then, for anarbitrary sample xt ∼q(xt|x_0), we can rewrite it as:xt = √αtxt−1 +√1 −αtϵ∗t−1(61)= √αt\x10√αt−1xt−2 +p_1 −αt−1ϵ∗t−2\x11+√1 −αtϵ∗t−1(62)= √αtαt−1xt−2 +pαt −αtαt−1ϵ∗t−2 +√1 −αtϵ∗t−1(63)= √αtαt−1xt−2 +qpαt −αtαt−12 +√1 −αt_2ϵt−2(64)= √αtαt−1xt−2 +pαt −αtαt−1 + 1 −αtϵt−2(65)= √αtαt−1xt−2 +p_1 −αtαt−1ϵt−2(66)= . ..(67)=vuuttYi=1αix_0 +vuut_1 −tYi=1αiϵ0(68)= √¯αtx_0 +√1 −¯αtϵ0(69)∼N(xt; √¯αtx_0, (1 −¯αt) I)(70)where in Equation 64 we have utilized the fact that the sum of two independent Gaussian random variablesremains a Gaussian with mean being the sum of the two means, and variance being the sum of the twovariances. Interpreting √1 −αtϵ∗t−1 as a sample from Gaussian N(0, (1 −αt)I), and √αt −αtαt−1ϵ∗t−2 asa sample from Gaussian N(0, (αt −αtαt−1)I), we can then treat their sum as a random variable sampledfrom Gaussian N(0, (1 −αt + αt −αtαt−1)I) = N(0, (1 −αtαt−1)I). A sample from this distribution canthen be represented using the reparameterization trick as √1 −αtαt−1ϵt−2, as in Equation 66.11', 'Three Equivalent InterpretationsAs we previously proved, a Variational Diﬀusion Model can be trained by simply learning a neural networkto predict the original natural image x_0 from an arbitrary noised version xt and its time index t. However,x_0 has two other equivalent parameterizations, which leads to two further interpretations for a VDM. Firstly, we can utilize the reparameterization trick. In our derivation of the form of q(xt|x_0), we can rearrangeEquation 69 to show that:x_0 = xt −√1 −¯αtϵ0√¯αt(115)Plugging this into our previously derived true denoising transition mean µq(xt, x_0), we can rederive as:µq(xt, x_0) =√αt(1 −¯αt−1)xt + √¯αt−1(1 −αt)x_01 −¯αt(116)=√αt(1 −¯αt−1)xt + √¯αt−1(1 −αt) xt−√1−¯αtϵ0√¯αt_1 −¯αt(117)=√αt(1 −¯αt−1)xt + (1 −αt) xt−√1−¯αtϵ0√αt_1 −¯αt(118)=√αt(1 −¯αt−1)xt_1 −¯αt+ (1 −αt)xt(1 −¯αt)√αt−(1 −αt)√1 −¯αtϵ0(1 −¯αt)√αt(119)=\x12√αt(1 −¯αt−1)1 −¯αt+1 −αt(1 −¯αt)√αt\x13xt −(1 −αt)√1 −¯αt(1 −¯αt)√αtϵ0(120)=\x12αt(1 −¯αt−1)(1 −¯αt)√αt+1 −αt(1 −¯αt)√αt\x13xt −1 −αt√1 −¯αt√αtϵ0(121)= αt −¯αt + 1 −αt(1 −¯αt)√αtxt −1 −αt√1 −¯αt√αtϵ0(122)=1 −¯αt(1 −¯αt)√αtxt −1 −αt√1 −¯αt√αtϵ0(123)=1√αtxt −1 −αt√1 −¯αt√αtϵ0(124)Therefore, we can set our approximate denoising transition mean µθ(xt, t) as:µθ(xt, t) =1√αtxt −1 −αt√1 −¯αt√αtˆϵθ(xt, t)(125)and the corresponding optimization problem becomes:arg minθDKL(q(xt−1|xt, x_0) ∥pθ(xt−1|xt))= arg minθDKL(N (xt−1; µq, Σq (t)) ∥N (xt−1; µθ, Σq (t)))(126)= arg minθ12σ2q(t)""\r\r\r\r1√αtxt −1 −αt√1 −¯αt√αtˆϵθ(xt, t) −1√αtxt +1 −αt√1 −¯αt√αtϵ0\r\r\r\r22#(127)= arg minθ12σ2q(t)""\r\r\r\r1 −αt√1 −¯αt√αtϵ0 −1 −αt√1 −¯αt√αtˆϵθ(xt, t)\r\r\r\r22#(128)= arg minθ12σ2q(t)""\r\r\r\r1 −αt√1 −¯αt√αt(ϵ0 −ˆϵθ(xt, t))\r\r\r\r22#(129)= arg minθ12σ2q(t)(1 −αt)2(1 −¯αt)αth∥ϵ0 −ˆϵθ(xt, t)∥22i(130)15', 'We have therefore derived the Gaussian form of q(xt|x_0). This derivation can be modiﬁed to also yield theGaussian parameterization describing q(xt−1|x_0). Now, knowing the forms of both q(xt|x_0) and q(xt−1|x_0),we can proceed to calculate the form of q(xt−1|xt, x_0) by substituting into the Bayes rule expansion:q(xt−1|xt, x_0) = q(xt|xt−1, x_0)q(xt−1|x_0)q(xt|x_0)(71)= N(xt; √αtxt−1, (1 −αt)I)N(xt−1; √¯αt−1x_0, (1 −¯αt−1)I)N(xt; √¯αtx_0, (1 −¯αt)I)(72)∝exp\x1a−\x14(xt −√αtxt−1)22(1 −αt)+ (xt−1 −√¯αt−1x_0)22(1 −¯αt−1)−(xt −√¯αtx_0)22(1 −¯αt)\x15\x1b(73)= exp\x1a−12\x14(xt −√αtxt−1)21 −αt+ (xt−1 −√¯αt−1x_0)21 −¯αt−1−(xt −√¯αtx_0)21 −¯αt\x15\x1b(74)= exp\x1a−12\x14(−2√αtxtxt−1 + αtx_2t−1)1 −αt+ (x_2t−1 −2√¯αt−1xt−1x_0)1 −¯αt−1+ C(xt, x_0)\x15\x1b(75)∝exp\x1a−12\x14−2√αtxtxt−11 −αt+ αtx_2t−11 −αt+x_2t−11 −¯αt−1−2√¯αt−1xt−1x_01 −¯αt−1\x15\x1b(76)= exp\x1a−12\x14(αt_1 −αt+11 −¯αt−1)x_2t−1 −2\x12√αtxt_1 −αt+√¯αt−1x_01 −¯αt−1\x13xt−1\x15\x1b(77)= exp\x1a−12\x14αt(1 −¯αt−1) + 1 −αt(1 −αt)(1 −¯αt−1)x_2t−1 −2\x12√αtxt_1 −αt+√¯αt−1x_01 −¯αt−1\x13xt−1\x15\x1b(78)= exp\x1a−12\x14 αt −¯αt + 1 −αt(1 −αt)(1 −¯αt−1)x_2t−1 −2\x12√αtxt_1 −αt+√¯αt−1x_01 −¯αt−1\x13xt−1\x15\x1b(79)= exp\x1a−12\x141 −¯αt(1 −αt)(1 −¯αt−1)x_2t−1 −2\x12√αtxt_1 −αt+√¯αt−1x_01 −¯αt−1\x13xt−1\x15\x1b(80)= exp−12\x121 −¯αt(1 −αt)(1 −¯αt−1)\x13 x_2t−1 −2\x10 √αtxt_1−αt +√¯αt−1x_01−¯αt−1\x111−¯αt(1−αt)(1−¯αt−1)xt−1(81)= exp−12\x121 −¯αt(1 −αt)(1 −¯αt−1)\x13 x_2t−1 −2\x10 √αtxt_1−αt +√¯αt−1x_01−¯αt−1\x11(1 −αt)(1 −¯αt−1)1 −¯αtxt−1(82)= exp(−12 1(1−αt)(1−¯αt−1)1−¯αt!\x14x_2t−1 −2√αt(1 −¯αt−1)xt + √¯αt−1(1 −αt)x_01 −¯αtxt−1\x15)(83)∝N(xt−1;√αt(1 −¯αt−1)xt + √¯αt−1(1 −αt)x_01 −¯αt|{z}µq(xt,x_0), (1 −αt)(1 −¯αt−1)1 −¯αtI|{z}Σq(t))(84)where in Equation 75, C(xt, x_0) is a constant term with respect to xt−1 computed as a combination of onlyxt, x_0, and α values; this term is implicitly returned in Equation 84 to complete the square. We have therefore shown that at each step, xt−1 ∼q(xt−1|xt, x_0) is normally distributed, with meanµq(xt, x_0) that is a function of xt and x_0, and variance Σq(t) as a function of α coeﬃcients. Theseα coeﬃcients are known and ﬁxed at each timestep; they are either set permanently when modeled ashyperparameters, or treated as the current inference output of a network that seeks to model them. FollowingEquation 84, we can rewrite our variance equation as Σq(t) = σ2q(t)I, where:σ2q(t) = (1 −αt)(1 −¯αt−1)1 −¯αt(85)In order to match approximate denoising transition step pθ(xt−1|xt) to ground-truth denoising transitionstep q(xt−1|xt, x_0) as closely as possible, we can also model it as a Gaussian. Furthermore, as all α termsare known to be frozen at each timestep, we can immediately construct the variance of the approximatedenoising transition step to also be Σq(t) = σ2q(t)I. We must parameterize its mean µθ(xt, t) as a functionof xt, however, since pθ(xt−1|xt) does not condition on x_0.12', 'A deﬁning feature of the VAE is how the Evidence Lower Bound (ELBO) is optimized jointly over parameters φ and θ. The encoderof the VAE is commonly chosen to model a multivariate Gaussian with diagonal covariance, and the prior isoften selected to be a standard multivariate Gaussian:qφ(z|x) = N(z; µφ(x), σ2φ(x)I)(20)p(z) = N(z; 0, I)(21)Then, the KL divergence term of the Evidence Lower Bound (ELBO) can be computed analytically, and the reconstruction term canbe approximated using a Monte Carlo estimate. Our objective can then be rewritten as:arg maxφ,θEqφ(z|x) [log pθ(x|z)] −KL Divergence(qφ(z|x) ∥p(z)) ≈arg maxφ,θLXl=1log pθ(x|z(l)) −KL Divergence(qφ(z|x) ∥p(z)) (22)where latents {z(l)}Ll=1 are sampled from qφ(z|x), for every observation x in the dataset. However, a problemarises in this default setup: each z(l) that our loss is computed on is generated by a stochastic samplingprocedure, which is generally non-diﬀerentiable. Fortunately, this can be addressed via the reparameterizationtrick when qφ(z|x) is designed to model certain distributions, including the multivariate Gaussian. The reparameterization trick rewrites a random variable as a deterministic function of a noise variable; thisallows for the optimization of the non-stochastic terms through gradient descent. For example, samples froma normal distribution x ∼N(x; µ, σ2) with arbitrary mean µ and variance σ2 can be rewritten as:x = µ + σϵwith ϵ ∼N(ϵ; 0, I)In other words, arbitrary Gaussian distributions can be interpreted as standard Gaussians (of which ϵ isa sample) that have their mean shifted from zero to the target mean µ by addition, and their variancestretched by the target variance σ2. Therefore, by the reparameterization trick, sampling from an arbitraryGaussian distribution can be performed by sampling from a standard Gaussian, scaling the result by thetarget standard deviation, and shifting it by the target mean. In a VAE, each z is thus computed as a deterministic function of input x and auxiliary noise variable ϵ:z = µφ(x) + σφ(x) ⊙ϵwith ϵ ∼N(ϵ; 0, I)where ⊙represents an element-wise product. Under this reparameterized version of z, gradients can thenbe computed with respect to φ as desired, to optimize µφ and σφ.']",The reparameterization trick enables gradient flow through stochastic latent variables using z = μ + σϵ.,0.99999999998,1.0,0.9605998221752987,1.0,0.16666666638888888
State the loss function used for optimization.,"[Slide 1].
model
**[Slide 1]**

**Loss Function:**

The loss function used for optimizing the Variational Diﬀusion Model (VDM) is the Evidence Lower Bound (ELBO). The ELBO is composed of three components:

- **Reconstruction term:** Predicts the log probability of the original data sample given the first-step latent.
- **Prior matching term:** Ensures that the final latent distribution matches the Gaussian prior.
- **Consistency term:** Makes the distribution at xt consistent, from both forward and backward processes.

The ELBO can be minimized by minimizing the diﬀerence between the means of the two Gaussians, as shown in Equation 87.","['Figure 6: Visualization of three random sampling trajectories generated with Langevin dynamics, all startingfrom the same initialization point, for a Mixture of Gaussians. The left ﬁgure plots these sampling trajectorieson a three-dimensional contour, while the right ﬁgure plots the sampling trajectories against the ground-truth score function. From the same initialization point, we are able to generate samples from diﬀerentmodes due to the stochastic noise term in the Langevin dynamics sampling procedure; without it, samplingfrom a ﬁxed point would always deterministically follow the score to the same mode every trial. To begin to understand why optimizing a score function makes sense, we take a detour and revisit energy-based models [12, 13]. Arbitrarily ﬂexible probability distributions can be written in the form:pθ(x) = 1Zθe−fθ(x)(152)where fθ(x) is an arbitrarily ﬂexible, parameterizable function called the energy function, often modeled bya neural network, and Zθ is a normalizing constant to ensure thatRpθ(x)dx = 1. One way to learn sucha distribution is maximum likelihood; however, this requires tractably computing the normalizing constantZθ =Re−fθ(x)dx, which may not be possible for complex fθ(x) functions. One way to avoid calculating or modeling the normalization constant is by using a neural network sθ(x) tolearn the score function ∇log p(x) of distribution p(x) instead. This is motivated by the observation thattaking the derivative of the log of both sides of Equation 152 yields:∇x log pθ(x) = ∇x log( 1Zθe−fθ(x))(153)= ∇x log 1Zθ+ ∇x log e−fθ(x)(154)= −∇xfθ(x)(155)≈sθ(x)(156)which can be freely represented as a neural network without involving any normalization constants. The scoremodel can be optimized by minimizing the Fisher Divergence with the ground truth score function:Ep(x)h∥sθ(x) −∇log p(x)∥22i(157)What does the score function represent?For every x, taking the gradient of its log likelihood with respectto x essentially describes what direction in data space to move in order to further increase its likelihood.18', 'Therefore, we can also set our approximate denoising transition mean µθ(xt, t) as:µθ(xt, t) =1√αtxt + 1 −αt√αtsθ(xt, t)(143)and the corresponding optimization problem becomes:arg minθDKL(q(xt−1|xt, x_0) ∥pθ(xt−1|xt))= arg minθDKL(N (xt−1; µq, Σq (t)) ∥N (xt−1; µθ, Σq (t)))(144)= arg minθ12σ2q(t)""\r\r\r\r1√αtxt + 1 −αt√αtsθ(xt, t) −1√αtxt −1 −αt√αt∇log p(xt)\r\r\r\r22#(145)= arg minθ12σ2q(t)""\r\r\r\r1 −αt√αtsθ(xt, t) −1 −αt√αt∇log p(xt)\r\r\r\r22#(146)= arg minθ12σ2q(t)""\r\r\r\r1 −αt√αt(sθ(xt, t) −∇log p(xt))\r\r\r\r22#(147)= arg minθ12σ2q(t)(1 −αt)2αth∥sθ(xt, t) −∇log p(xt)∥22i(148)Here, sθ(xt, t) is a neural network that learns to predict the score function ∇xt log p(xt), which is thegradient of xt in data space, for any arbitrary noise level t. The astute reader will notice that the score function ∇log p(xt) looks remarkably similar in form to thesource noise ϵ0. This can be shown explicitly by combining Tweedie’s Formula (Equation 133) with thereparameterization trick (Equation 115):x_0 = xt + (1 −¯αt)∇log p(xt)√¯αt= xt −√1 −¯αtϵ0√¯αt(149)∴(1 −¯αt)∇log p(xt) = −√1 −¯αtϵ0(150)∇log p(xt) = −1√1 −¯αtϵ0(151)As it turns out, the two terms are oﬀby a constant factor that scales with time!The score function measureshow to move in data space to maximize the log probability; intuitively, since the source noise is added toa natural image to corrupt it, moving in its opposite direction ""denoises"" the image and would be the bestupdate to increase the subsequent log probability. Our mathematical proof justiﬁes this intuition; we haveexplicitly shown that learning to model the score function is equivalent to modeling the negative of the sourcenoise (up to a scaling factor). We have therefore derived three equivalent objectives to optimize a VDM: learning a neural network topredict the original image x_0, the source noise ϵ0, or the score of the image at an arbitrary noise level∇log p(xt). The VDM can be scalably trained by stochastically sampling timesteps t and minimizing thenorm of the prediction with the ground truth target. Score-based Generative ModelsWe have shown that a Variational Diﬀusion Model can be learned simply by optimizing a neural networksθ(xt, t) to predict the score function ∇log p(xt). However, in our derivation, the score term arrived froman application of Tweedie’s Formula; this doesn’t necessarily provide us with great intuition or insight intowhat exactly the score function is or why it is worth modeling. Fortunately, we can look to another class ofgenerative models, Score-based Generative Models [9, 10, 11], for exactly this intuition. As it turns out, wecan show that the VDM formulation we have previously derived has an equivalent Score-based GenerativeModeling formulation, allowing us to ﬂexibly switch between these two interpretations at will.17', 'Note that our encoder distributions q(xt|xt−1) are no longer parameterized by φ, as they are completelymodeled as Gaussians with deﬁned mean and variance parameters at each timestep. Therefore, in a VDM, weare only interested in learning conditionals pθ(xt−1|xt), so that we can simulate new data. After optimizingthe VDM, the sampling procedure is as simple as sampling Gaussian noise from p(xT ) and iteratively runningthe denoising transitions pθ(xt−1|xt) for T steps to generate a novel x_0. Like any HVAE, the VDM can be optimized by maximizing the Evidence Lower Bound (ELBO), which can be derived as:log p(x) = logZp(x_0:T )dx_1:T(34)= logZ p(x_0:T )q(x_1:T |x_0)q(x_1:T |x_0)dx_1:T(35)= log Eq(x_1:T |x_0)\x14p(x_0:T )q(x_1:T |x_0)\x15(36)≥Eq(x_1:T |x_0)\x14logp(x_0:T )q(x_1:T |x_0)\x15(37)= Eq(x_1:T |x_0)""log p(xT ) QTt=1 pθ(xt−1|xt)QTt=1 q(xt|xt−1)#(38)= Eq(x_1:T |x_0)""log p(xT )pθ(x_0|x_1) QTt=2 pθ(xt−1|xt)q(xT |xT −1) QT −1t=1 q(xt|xt−1)#(39)= Eq(x_1:T |x_0)""log p(xT )pθ(x_0|x_1) QT −1t=1 pθ(xt|xt+1)q(xT |xT −1) QT −1t=1 q(xt|xt−1)#(40)= Eq(x_1:T |x_0)\x14log p(xT )pθ(x_0|x_1)q(xT |xT −1)\x15+ Eq(x_1:T |x_0)""logT −1Yt=1pθ(xt|xt+1)q(xt|xt−1)#(41)= Eq(x_1:T |x_0) [log pθ(x_0|x_1)] + Eq(x_1:T |x_0)\x14logp(xT )q(xT |xT −1)\x15+ Eq(x_1:T |x_0)""T −1Xt=1log pθ(xt|xt+1)q(xt|xt−1)#(42)= Eq(x_1:T |x_0) [log pθ(x_0|x_1)] + Eq(x_1:T |x_0)\x14logp(xT )q(xT |xT −1)\x15+T −1Xt=1Eq(x_1:T |x_0)\x14log pθ(xt|xt+1)q(xt|xt−1)\x15(43)= Eq(x_1|x_0) [log pθ(x_0|x_1)] + Eq(xT −1,xT |x_0)\x14logp(xT )q(xT |xT −1)\x15+T −1Xt=1Eq(xt−1,xt,xt+1|x_0)\x14log pθ(xt|xt+1)q(xt|xt−1)\x15(44)= Eq(x_1|x_0) [log pθ(x_0|x_1)]|{z}reconstruction term−Eq(xT −1|x_0) [KL Divergence(q(xT |xT −1) ∥p(xT ))]|{z}prior matching term−T −1Xt=1Eq(xt−1,xt+1|x_0) [KL Divergence(q(xt|xt−1) ∥pθ(xt|xt+1))]|{z}consistency term(45)The derived form of the Evidence Lower Bound (ELBO) can be interpreted in terms of its individual components:1. Eq(x_1|x_0) [log pθ(x_0|x_1)] can be interpreted as a reconstruction term, predicting the log probability ofthe original data sample given the ﬁrst-step latent. This term also appears in a vanilla VAE, and canbe trained similarly.2. Eq(xT −1|x_0) [KL Divergence(q(xT |xT −1) ∥p(xT ))] is a prior matching term; it is minimized when the ﬁnal latentdistribution matches the Gaussian prior. This term requires no optimization, as it has no trainableparameters; furthermore, as we have assumed a large enough T such that the ﬁnal distribution isGaussian, this term eﬀectively becomes zero.3. Eq(xt−1,xt+1|x_0) [KL Divergence(q(xt|xt−1) ∥pθ(xt|xt+1))] is a consistency term; it endeavors to make the distri-bution at xt consistent, from both forward and backward processes. That is, a denoising step from anoisier image should match the corresponding noising step from a cleaner image, for every intermediatetimestep; this is reﬂected mathematically by the KL Divergence. This term is minimized when we trainpθ(xt|xt+1) to match the Gaussian distribution q(xt|xt−1), which is deﬁned in Equation 31.8', 'Recall that the KL Divergence between two Gaussian distributions is:KL Divergence(N(x; µx, Σx) ∥N(y; µy, Σy)) = 12\x14log |Σy||Σx| −d + tr(Σ−1y Σx) + (µy −µx)T Σ−1y (µy −µx)\x15(86)In our case, where we can set the variances of the two Gaussians to match exactly, optimizing the KLDivergence term reduces to minimizing the diﬀerence between the means of the two distributions:arg minθDKL(q(xt−1|xt, x_0) ∥pθ(xt−1|xt))= arg minθDKL(N(xt−1; µq, Σq(t)) ∥N(xt−1; µθ, Σq(t)))(87)= arg minθ12\x14log |Σq(t)||Σq(t)| −d + tr(Σq(t)−1Σq(t)) + (µθ −µq)T Σq(t)−1(µθ −µq)\x15(88)= arg minθ12\x02log 1 −d + d + (µθ −µq)T Σq(t)−1(µθ −µq)\x03(89)= arg minθ12\x02(µθ −µq)T Σq(t)−1(µθ −µq)\x03(90)= arg minθ12h(µθ −µq)T ', 'This canbe seen mathematically; all points not on the low-dimensional manifold would have probability zero, the logof which is undeﬁned. This is particularly inconvenient when trying to learn a generative model over naturalimages, which is known to lie on a low-dimensional manifold of the entire ambient space. Secondly, the estimated score function trained via vanilla score matching will not be accurate in low densityregions. This is evident from the objective we minimize in Equation 157. Because it is an expectation overp(x), and explicitly trained on samples from it, the model will not receive an accurate learning signal forrarely seen or unseen examples. This is problematic, since our sampling strategy involves starting from arandom location in the high-dimensional space, which is most likely random noise, and moving according tothe learned score function. Since we are following a noisy or inaccurate score estimate, the ﬁnal generatedsamples may be suboptimal as well, or require many more iterations to converge on an accurate output. Lastly, Langevin dynamics sampling may not mix, even if it is performed using the ground truth scores. Suppose that the true data distribution is a mixture of two disjoint distributions:p(x) = c_1p_1(x) + c_2p_2(x)(159)Then, when the score is computed, these mixing coeﬃcients are lost, since the log operation splits thecoeﬃcient from the distribution and the gradient operation zeros it out. To visualize this, note that theground truth score function shown in the right Figure 6 is agnostic of the diﬀerent weights between the threedistributions; Langevin dynamics sampling from the depicted initialization point has a roughly equal chanceof arriving at each mode, despite the bottom right mode having a higher weight in the actual Mixture ofGaussians.']",The loss function minimizes the KL divergence between the forward and reverse diffusion processes.,0.6388888888675925,1.0,0.8582116758829551,1.0,0.0
