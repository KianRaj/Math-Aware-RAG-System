Paper_ID,Generated_Script,Reference_Script,Source_Context,Semantic_Overlap_Factuality_Proxy_OpenAI,Citation_Coverage,ROUGE_rouge1_F1,ROUGE_rouge2_F1,ROUGE_rougeL_F1,BERTScore_P,BERTScore_R,BERTScore_F1,Prompt_Type,Rater_ID,Audience_Appropriateness_Rating,Factuality_Rating,Helpfulness_Rating
P1_VDM,"The ELBO (Evidence Lower Bound Objective) aims to maximize the likelihood of the observed data under the variational approximation. It consists of two main terms[Paper 4]The first term, $$log p(x/z)/q(z/x)=Eq (zx)[logp (xz)]$$, measures the reconstruction likelihood of the decoder from the variational distribution, ensuring that the learned distribution accurately models the original data.[Paper 4]The second term, DKL(q (zx) p(z)), measures the similarity of the learned variational distribution to the prior belief held over latent variables. Minimizing this term encourages the encoder to learn a distribution rather than collapsing into a Dirac delta function.[Page 4]."," The VAE therefore utilizes the reparam-eterization trick and Monte Carlo estimates to optimize the Evidence Lower Bound (ELBO) jointly over φ and θ. After training a VAE, generating new data can be  [Page 4].  A deﬁning feature of the VAE is how the Evidence Lower Bound (ELBO) is optimized jointly over parameters φ and θ. The encoderof the VAE is commonly chosen to model a multivariate Gaussian with [Page 4].  The VAE therefore utilizes the reparam-eterization trick and Monte Carlo estimates to optimize the Evidence Lower Bound (ELBO) jointly over φ and θ. After training a VAE, generating new data can be [Page 4]"," In the default formulation of the Variational Autoencoder (VAE) [1], we directly maximize the ELBO. This approach is variational, because we optimize for the best q (zx) amongst a family of potential posterior distributions parameterized by . It is called an autoencoder because it is reminiscent of a traditional au toencoder model, where input data is trained to predict itself after undergoing an intermediate bottlenecking representation step.[Page 4]
 Eq (zx) log p(x/z)/q(z/x)=Eq (zx)[logp (xz)]-DKL(q (zx) p(z)).  the first term measures the reconstruction likelihood of the decoder from our variational distribution this ensures that the learned distribution is modeling effective latents that the original data can be regenerated from. The second term measures how similar the learned variational distribution is to a prior belief held over latent variables. Minimizing this term encourages the encoder to actually learn a distribution rather than collapse into a Dirac delta function.[Page 4]
 Maximizing the ELBO is thus equivalent to maximizing its first term and minimizing its second term.[Page 4]",0.0,0.3333,0.2679,0.0483,0.1914,0.8107,0.8119,0.8113,Graduate Student,Rater_1,,,
P1_VDM,"The ELBO (Evidence Lower Bound Objective) aims to maximize the likelihood of the observed data under the variational approximation. It consists of two main terms[Paper 4]The first term, $$log p(x/z)/q(z/x)=Eq (zx)[logp (xz)]$$, measures the reconstruction likelihood of the decoder from the variational distribution, ensuring that the learned distribution accurately models the original data.[Paper 4]The second term, DKL(q (zx) p(z)), measures the similarity of the learned variational distribution to the prior belief held over latent variables. Minimizing this term encourages the encoder to learn a distribution rather than collapsing into a Dirac delta function.[Page 4]."," The VAE therefore utilizes the reparam-eterization trick and Monte Carlo estimates to optimize the Evidence Lower Bound (ELBO) jointly over φ and θ. After training a VAE, generating new data can be  [Page 4].  A deﬁning feature of the VAE is how the Evidence Lower Bound (ELBO) is optimized jointly over parameters φ and θ. The encoderof the VAE is commonly chosen to model a multivariate Gaussian with [Page 4].  The VAE therefore utilizes the reparam-eterization trick and Monte Carlo estimates to optimize the Evidence Lower Bound (ELBO) jointly over φ and θ. After training a VAE, generating new data can be [Page 4]"," In the default formulation of the Variational Autoencoder (VAE) [1], we directly maximize the ELBO. This approach is variational, because we optimize for the best q (zx) amongst a family of potential posterior distributions parameterized by . It is called an autoencoder because it is reminiscent of a traditional au toencoder model, where input data is trained to predict itself after undergoing an intermediate bottlenecking representation step.[Page 4]
 Eq (zx) log p(x/z)/q(z/x)=Eq (zx)[logp (xz)]-DKL(q (zx) p(z)).  the first term measures the reconstruction likelihood of the decoder from our variational distribution this ensures that the learned distribution is modeling effective latents that the original data can be regenerated from. The second term measures how similar the learned variational distribution is to a prior belief held over latent variables. Minimizing this term encourages the encoder to actually learn a distribution rather than collapse into a Dirac delta function.[Page 4]
 Maximizing the ELBO is thus equivalent to maximizing its first term and minimizing its second term.[Page 4]",0.0,0.3333,0.2679,0.0483,0.1914,0.8107,0.8119,0.8113,Graduate Student,Rater_2,,,
P1_VDM,"The ELBO (Evidence Lower Bound Objective) aims to maximize the likelihood of the observed data under the variational approximation. It consists of two main terms[Paper 4]The first term, $$log p(x/z)/q(z/x)=Eq (zx)[logp (xz)]$$, measures the reconstruction likelihood of the decoder from the variational distribution, ensuring that the learned distribution accurately models the original data.[Paper 4]The second term, DKL(q (zx) p(z)), measures the similarity of the learned variational distribution to the prior belief held over latent variables. Minimizing this term encourages the encoder to learn a distribution rather than collapsing into a Dirac delta function.[Page 4]."," The VAE therefore utilizes the reparam-eterization trick and Monte Carlo estimates to optimize the Evidence Lower Bound (ELBO) jointly over φ and θ. After training a VAE, generating new data can be  [Page 4].  A deﬁning feature of the VAE is how the Evidence Lower Bound (ELBO) is optimized jointly over parameters φ and θ. The encoderof the VAE is commonly chosen to model a multivariate Gaussian with [Page 4].  The VAE therefore utilizes the reparam-eterization trick and Monte Carlo estimates to optimize the Evidence Lower Bound (ELBO) jointly over φ and θ. After training a VAE, generating new data can be [Page 4]"," In the default formulation of the Variational Autoencoder (VAE) [1], we directly maximize the ELBO. This approach is variational, because we optimize for the best q (zx) amongst a family of potential posterior distributions parameterized by . It is called an autoencoder because it is reminiscent of a traditional au toencoder model, where input data is trained to predict itself after undergoing an intermediate bottlenecking representation step.[Page 4]
 Eq (zx) log p(x/z)/q(z/x)=Eq (zx)[logp (xz)]-DKL(q (zx) p(z)).  the first term measures the reconstruction likelihood of the decoder from our variational distribution this ensures that the learned distribution is modeling effective latents that the original data can be regenerated from. The second term measures how similar the learned variational distribution is to a prior belief held over latent variables. Minimizing this term encourages the encoder to actually learn a distribution rather than collapse into a Dirac delta function.[Page 4]
 Maximizing the ELBO is thus equivalent to maximizing its first term and minimizing its second term.[Page 4]",0.0,0.3333,0.2679,0.0483,0.1914,0.8107,0.8119,0.8113,Graduate Student,Rater_3,,,
P2_MFP3D,"The MFP_3D dataset is a novel 3D food dataset designed to advance food computing and 3D vision research. This dataset consists of meticulously scanned and labeled 3D food objects across 131 categories, featuring detailed nutrition information, weight, and food codes linked to a comprehensive nutrition database. The dataset offers a robust benchmark for developing and evaluating 3D vision algorithms for food portion estimation, synthetic eating occasion simulation, and 3D food object generation. Experimental results demonstrate the strong capabilities of MFP_3D in enhancing food portion estimation algorithms and highlight the need for well-constructed food datasets in this field[page 1]. Large-scale 3D Food Dataset: MFP_3D contains 743 food objects with diverse textures, shapes, and sizes.[page 1] Multimodal Data: MFP_3D provides RGBD videos, segmentation masks, depth images, and other modalities to facilitate various research tasks.[Page 1]. Benchmarking and Evaluation: MFP_3D offers a robust benchmark for evaluating 3D vision algorithms for food portion estimation. [Table 1].","Inthe Nutrition_5K dataset, food items are mixed together with-out segmentation masks, making it infeasible to performnutrition and geometric modeling for individual food items. The NutritionVerse_3D[page 2]. 3D MeshRGBD VideoDepth MapFigure 1. MetaFood_3D is a real-scan 3D food dataset featuring diverse ready-to-eat 3D textured meshes, 720-degree RGBD videocaptures, and rich nutrition value annotations [page 1]. A comparison offood portion size estimation using geometric models anddepth images. Proceedings of the 2016 IEEE InternationalConference on Image Processing, pages 26–30, 2016. 3, 8[12] Anthony G. [page 8] ","Food portion estimation is crucial for monitoring health and tracking dietary intake. Image-based dietary assessment, which involves analyzing eating occasion images using computer vision techniques, is increasingly replacing traditional methods such as 24-hour recalls. However, accurately estimating the nutritional content from images remains challenging due to the loss of 3D information when projecting to the 2D image plane. Existing portion estimation methods are challenging to deploy in real-world scenarios due to their reliance on specific requirements, such as physical reference objects, high-quality depth information, or multi-view images and videos. In this paper, we introduce MFP3D, a new framework for accurate food portion estimation using only a single monocular image. Specifically, MFP3D consists of three key modules: (1) a 3D Reconstruction Module that generates a 3D point cloud representation of the food from the 2D image, (2) a Feature Extraction Module that extracts and concatenates features from both the 3D point cloud and the 2D RGB image, and (3) a Portion Regression Module that employs a deep regression model to estimate the food’s volume and energy content based on the extracted features. Our MFP3D is evaluated on MetaFood3D dataset, demonstrating its significant improvement in accurate portion estimation over existing methods. Keywords: Food Portion Estimation · 3D Point Cloud · Monocular Image · Multimodality Model [Abstract].",0.0,0.1429,0.3203,0.0394,0.1719,0.8523,0.8442,0.8482,Press Release,Rater_1,,,
P2_MFP3D,"The MFP_3D dataset is a novel 3D food dataset designed to advance food computing and 3D vision research. This dataset consists of meticulously scanned and labeled 3D food objects across 131 categories, featuring detailed nutrition information, weight, and food codes linked to a comprehensive nutrition database. The dataset offers a robust benchmark for developing and evaluating 3D vision algorithms for food portion estimation, synthetic eating occasion simulation, and 3D food object generation. Experimental results demonstrate the strong capabilities of MFP_3D in enhancing food portion estimation algorithms and highlight the need for well-constructed food datasets in this field[page 1]. Large-scale 3D Food Dataset: MFP_3D contains 743 food objects with diverse textures, shapes, and sizes.[page 1] Multimodal Data: MFP_3D provides RGBD videos, segmentation masks, depth images, and other modalities to facilitate various research tasks.[Page 1]. Benchmarking and Evaluation: MFP_3D offers a robust benchmark for evaluating 3D vision algorithms for food portion estimation. [Table 1].","Inthe Nutrition_5K dataset, food items are mixed together with-out segmentation masks, making it infeasible to performnutrition and geometric modeling for individual food items. The NutritionVerse_3D[page 2]. 3D MeshRGBD VideoDepth MapFigure 1. MetaFood_3D is a real-scan 3D food dataset featuring diverse ready-to-eat 3D textured meshes, 720-degree RGBD videocaptures, and rich nutrition value annotations [page 1]. A comparison offood portion size estimation using geometric models anddepth images. Proceedings of the 2016 IEEE InternationalConference on Image Processing, pages 26–30, 2016. 3, 8[12] Anthony G. [page 8] ","Food portion estimation is crucial for monitoring health and tracking dietary intake. Image-based dietary assessment, which involves analyzing eating occasion images using computer vision techniques, is increasingly replacing traditional methods such as 24-hour recalls. However, accurately estimating the nutritional content from images remains challenging due to the loss of 3D information when projecting to the 2D image plane. Existing portion estimation methods are challenging to deploy in real-world scenarios due to their reliance on specific requirements, such as physical reference objects, high-quality depth information, or multi-view images and videos. In this paper, we introduce MFP3D, a new framework for accurate food portion estimation using only a single monocular image. Specifically, MFP3D consists of three key modules: (1) a 3D Reconstruction Module that generates a 3D point cloud representation of the food from the 2D image, (2) a Feature Extraction Module that extracts and concatenates features from both the 3D point cloud and the 2D RGB image, and (3) a Portion Regression Module that employs a deep regression model to estimate the food’s volume and energy content based on the extracted features. Our MFP3D is evaluated on MetaFood3D dataset, demonstrating its significant improvement in accurate portion estimation over existing methods. Keywords: Food Portion Estimation · 3D Point Cloud · Monocular Image · Multimodality Model [Abstract].",0.0,0.1429,0.3203,0.0394,0.1719,0.8523,0.8442,0.8482,Press Release,Rater_2,,,
P2_MFP3D,"The MFP_3D dataset is a novel 3D food dataset designed to advance food computing and 3D vision research. This dataset consists of meticulously scanned and labeled 3D food objects across 131 categories, featuring detailed nutrition information, weight, and food codes linked to a comprehensive nutrition database. The dataset offers a robust benchmark for developing and evaluating 3D vision algorithms for food portion estimation, synthetic eating occasion simulation, and 3D food object generation. Experimental results demonstrate the strong capabilities of MFP_3D in enhancing food portion estimation algorithms and highlight the need for well-constructed food datasets in this field[page 1]. Large-scale 3D Food Dataset: MFP_3D contains 743 food objects with diverse textures, shapes, and sizes.[page 1] Multimodal Data: MFP_3D provides RGBD videos, segmentation masks, depth images, and other modalities to facilitate various research tasks.[Page 1]. Benchmarking and Evaluation: MFP_3D offers a robust benchmark for evaluating 3D vision algorithms for food portion estimation. [Table 1].","Inthe Nutrition_5K dataset, food items are mixed together with-out segmentation masks, making it infeasible to performnutrition and geometric modeling for individual food items. The NutritionVerse_3D[page 2]. 3D MeshRGBD VideoDepth MapFigure 1. MetaFood_3D is a real-scan 3D food dataset featuring diverse ready-to-eat 3D textured meshes, 720-degree RGBD videocaptures, and rich nutrition value annotations [page 1]. A comparison offood portion size estimation using geometric models anddepth images. Proceedings of the 2016 IEEE InternationalConference on Image Processing, pages 26–30, 2016. 3, 8[12] Anthony G. [page 8] ","Food portion estimation is crucial for monitoring health and tracking dietary intake. Image-based dietary assessment, which involves analyzing eating occasion images using computer vision techniques, is increasingly replacing traditional methods such as 24-hour recalls. However, accurately estimating the nutritional content from images remains challenging due to the loss of 3D information when projecting to the 2D image plane. Existing portion estimation methods are challenging to deploy in real-world scenarios due to their reliance on specific requirements, such as physical reference objects, high-quality depth information, or multi-view images and videos. In this paper, we introduce MFP3D, a new framework for accurate food portion estimation using only a single monocular image. Specifically, MFP3D consists of three key modules: (1) a 3D Reconstruction Module that generates a 3D point cloud representation of the food from the 2D image, (2) a Feature Extraction Module that extracts and concatenates features from both the 3D point cloud and the 2D RGB image, and (3) a Portion Regression Module that employs a deep regression model to estimate the food’s volume and energy content based on the extracted features. Our MFP3D is evaluated on MetaFood3D dataset, demonstrating its significant improvement in accurate portion estimation over existing methods. Keywords: Food Portion Estimation · 3D Point Cloud · Monocular Image · Multimodality Model [Abstract].",0.0,0.1429,0.3203,0.0394,0.1719,0.8523,0.8442,0.8482,Press Release,Rater_3,,,
P3_MetaFood3D,"The MetaFood_3D dataset is a novel 3D food dataset designed to advance food computing and 3D vision research. This dataset includes 743 meticulously scanned and labeled 3D food objects across 131 categories, featuring detailed nutrition information, weight, and food codes linked to a comprehensive nutrition database. The dataset emphasizes intra-class diversity and includes rich modalities such as textured mesh files, RGB-D videos, and segmentation masks. Experimental results demonstrate the strong capabilities of the dataset in enhancing food portion estimation algorithms and generating synthetic eating occasion data.[Page 1]. Large-scale 3D Food Dataset: MetaFood_3D is the largest and most diverse 3D food dataset with nutrition values, providing a robust benchmark for developing and evaluating 3D vision algorithms. Detailed Nutrition Annotations: The dataset includes detailed nutrition annotations for each food object, enabling accurate energy calculations and dietary assessment.Rich Modalities: MetaFood_3D includes various modalities such as textured mesh files, RGB-D videos, and segmentation masks, providing a rich foundation for food-related tasks","MetaFood_3D 3D Food Dataset with Nutrition ValuesYuhao Chen_2Jiangpeng He_1†Gautham Vinod_1Siddeshwar Raghavan_1Chris Czarnecki_2*Jinge Ma_1Talha Ibn Mahmud_1Bruce Coburn_1Dayou Mao_2Saeejith [page 1] 3D MeshRGBD VideoDepth MapFigure 1. MetaFood_3D is a real-scan 3D food dataset featuring diverse ready-to-eat 3D textured meshes, 720-degree RGBD videocaptures, and rich nutrition value [page 1]. Furthermore,there is a shortage of benchmark 3D food datasets featur-ing diverse intra-class variation. For instance, the OmniOb-ject_3D dataset [88] includes 2,837 food objects, but the selec-tion [page 1] ","Food computing is both important and challenging in computer vision (CV). It significantly contributes to the development of CV algorithms due to its frequent presence in datasets across various applications, ranging from classification and instance segmentation to 3D reconstruction. The polymorphic shapes and textures of food, coupled with high variation in forms and vast multimodal information, including language descriptions and nutritional data, make food computing a complex and demanding task for modern CV algorithms. 3D food modeling is a new frontier for addressing food related problems, due to its inherent capability to deal with random camera views and its straightforward representation for calculating food portion size. However, the primary hurdle in the development of algorithms for food object analysis is the lack of nutrition values in existing 3D datasets. Moreover, in the broader field of 3D research, there is a critical need for domain-specific test datasets. To bridge the gap between general 3D vision and food computing research, we introduce MetaFood3D. This dataset consists of 743 meticulously scanned and labeled 3D food objects across 131 categories, featuring detailed nutrition information, weight, and food codes linked to a comprehensive nutrition database. Our MetaFood3D dataset emphasizes intra-class diversity and includes rich modalities such as textured mesh files, RGB-D videos, and segmentation masks. Experimental results demonstrate our dataset’s strong capabilities in enhancing food portion estimation algorithms, highlight the gap between video captures and 3D scanned data, and showcase the strengths of MetaFood3D in generating synthetic eating occasion data and 3D food objects [Page 1].
The dataset features detailed nutrition information, weight, and food codes [Page 1].
It includes rich modalities such as textured mesh files, RGB-D videos, and segmentation masks [Page 1].
We emphasize intra-class diversity by collecting foods with varying appearances [Page 1].",0.0,0.1667,0.3547,0.1369,0.1962,0.8596,0.8021,0.8299,Technical Slide,Rater_1,,,
P3_MetaFood3D,"The MetaFood_3D dataset is a novel 3D food dataset designed to advance food computing and 3D vision research. This dataset includes 743 meticulously scanned and labeled 3D food objects across 131 categories, featuring detailed nutrition information, weight, and food codes linked to a comprehensive nutrition database. The dataset emphasizes intra-class diversity and includes rich modalities such as textured mesh files, RGB-D videos, and segmentation masks. Experimental results demonstrate the strong capabilities of the dataset in enhancing food portion estimation algorithms and generating synthetic eating occasion data.[Page 1]. Large-scale 3D Food Dataset: MetaFood_3D is the largest and most diverse 3D food dataset with nutrition values, providing a robust benchmark for developing and evaluating 3D vision algorithms. Detailed Nutrition Annotations: The dataset includes detailed nutrition annotations for each food object, enabling accurate energy calculations and dietary assessment.Rich Modalities: MetaFood_3D includes various modalities such as textured mesh files, RGB-D videos, and segmentation masks, providing a rich foundation for food-related tasks","MetaFood_3D 3D Food Dataset with Nutrition ValuesYuhao Chen_2Jiangpeng He_1†Gautham Vinod_1Siddeshwar Raghavan_1Chris Czarnecki_2*Jinge Ma_1Talha Ibn Mahmud_1Bruce Coburn_1Dayou Mao_2Saeejith [page 1] 3D MeshRGBD VideoDepth MapFigure 1. MetaFood_3D is a real-scan 3D food dataset featuring diverse ready-to-eat 3D textured meshes, 720-degree RGBD videocaptures, and rich nutrition value [page 1]. Furthermore,there is a shortage of benchmark 3D food datasets featur-ing diverse intra-class variation. For instance, the OmniOb-ject_3D dataset [88] includes 2,837 food objects, but the selec-tion [page 1] ","Food computing is both important and challenging in computer vision (CV). It significantly contributes to the development of CV algorithms due to its frequent presence in datasets across various applications, ranging from classification and instance segmentation to 3D reconstruction. The polymorphic shapes and textures of food, coupled with high variation in forms and vast multimodal information, including language descriptions and nutritional data, make food computing a complex and demanding task for modern CV algorithms. 3D food modeling is a new frontier for addressing food related problems, due to its inherent capability to deal with random camera views and its straightforward representation for calculating food portion size. However, the primary hurdle in the development of algorithms for food object analysis is the lack of nutrition values in existing 3D datasets. Moreover, in the broader field of 3D research, there is a critical need for domain-specific test datasets. To bridge the gap between general 3D vision and food computing research, we introduce MetaFood3D. This dataset consists of 743 meticulously scanned and labeled 3D food objects across 131 categories, featuring detailed nutrition information, weight, and food codes linked to a comprehensive nutrition database. Our MetaFood3D dataset emphasizes intra-class diversity and includes rich modalities such as textured mesh files, RGB-D videos, and segmentation masks. Experimental results demonstrate our dataset’s strong capabilities in enhancing food portion estimation algorithms, highlight the gap between video captures and 3D scanned data, and showcase the strengths of MetaFood3D in generating synthetic eating occasion data and 3D food objects [Page 1].
The dataset features detailed nutrition information, weight, and food codes [Page 1].
It includes rich modalities such as textured mesh files, RGB-D videos, and segmentation masks [Page 1].
We emphasize intra-class diversity by collecting foods with varying appearances [Page 1].",0.0,0.1667,0.3547,0.1369,0.1962,0.8596,0.8021,0.8299,Technical Slide,Rater_2,,,
P3_MetaFood3D,"The MetaFood_3D dataset is a novel 3D food dataset designed to advance food computing and 3D vision research. This dataset includes 743 meticulously scanned and labeled 3D food objects across 131 categories, featuring detailed nutrition information, weight, and food codes linked to a comprehensive nutrition database. The dataset emphasizes intra-class diversity and includes rich modalities such as textured mesh files, RGB-D videos, and segmentation masks. Experimental results demonstrate the strong capabilities of the dataset in enhancing food portion estimation algorithms and generating synthetic eating occasion data.[Page 1]. Large-scale 3D Food Dataset: MetaFood_3D is the largest and most diverse 3D food dataset with nutrition values, providing a robust benchmark for developing and evaluating 3D vision algorithms. Detailed Nutrition Annotations: The dataset includes detailed nutrition annotations for each food object, enabling accurate energy calculations and dietary assessment.Rich Modalities: MetaFood_3D includes various modalities such as textured mesh files, RGB-D videos, and segmentation masks, providing a rich foundation for food-related tasks","MetaFood_3D 3D Food Dataset with Nutrition ValuesYuhao Chen_2Jiangpeng He_1†Gautham Vinod_1Siddeshwar Raghavan_1Chris Czarnecki_2*Jinge Ma_1Talha Ibn Mahmud_1Bruce Coburn_1Dayou Mao_2Saeejith [page 1] 3D MeshRGBD VideoDepth MapFigure 1. MetaFood_3D is a real-scan 3D food dataset featuring diverse ready-to-eat 3D textured meshes, 720-degree RGBD videocaptures, and rich nutrition value [page 1]. Furthermore,there is a shortage of benchmark 3D food datasets featur-ing diverse intra-class variation. For instance, the OmniOb-ject_3D dataset [88] includes 2,837 food objects, but the selec-tion [page 1] ","Food computing is both important and challenging in computer vision (CV). It significantly contributes to the development of CV algorithms due to its frequent presence in datasets across various applications, ranging from classification and instance segmentation to 3D reconstruction. The polymorphic shapes and textures of food, coupled with high variation in forms and vast multimodal information, including language descriptions and nutritional data, make food computing a complex and demanding task for modern CV algorithms. 3D food modeling is a new frontier for addressing food related problems, due to its inherent capability to deal with random camera views and its straightforward representation for calculating food portion size. However, the primary hurdle in the development of algorithms for food object analysis is the lack of nutrition values in existing 3D datasets. Moreover, in the broader field of 3D research, there is a critical need for domain-specific test datasets. To bridge the gap between general 3D vision and food computing research, we introduce MetaFood3D. This dataset consists of 743 meticulously scanned and labeled 3D food objects across 131 categories, featuring detailed nutrition information, weight, and food codes linked to a comprehensive nutrition database. Our MetaFood3D dataset emphasizes intra-class diversity and includes rich modalities such as textured mesh files, RGB-D videos, and segmentation masks. Experimental results demonstrate our dataset’s strong capabilities in enhancing food portion estimation algorithms, highlight the gap between video captures and 3D scanned data, and showcase the strengths of MetaFood3D in generating synthetic eating occasion data and 3D food objects [Page 1].
The dataset features detailed nutrition information, weight, and food codes [Page 1].
It includes rich modalities such as textured mesh files, RGB-D videos, and segmentation masks [Page 1].
We emphasize intra-class diversity by collecting foods with varying appearances [Page 1].",0.0,0.1667,0.3547,0.1369,0.1962,0.8596,0.8021,0.8299,Technical Slide,Rater_3,,,
